# -*- coding: utf-8 -*-
"""Website classifier 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j6f0aAQqWM4pDPKTyX6263INju2OC0vL

# "I-Detox" dealing with internet addiction

1. The import pandas portion of the code tells Python to bring the pandas data analysis library into our current environment.
2. The import numpy portion of the code tells Python to bring the NumPy library into your current environment.
3. Countvectorizer is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.
4. The train_test_split() method is used to split our data into train and test sets.We need to divide our data into features (X) and labels (y).
5. The multinomial Naive Bayes classifier is suitable for text classification using word counts
6. Beautiful Soup is a library that makes it easy to scrape information from web pages. It uses an HTML or XML parser, providing meta-data for iterating, searching, and modifying the parse tree.
7. Requests will fetch a page, and BeautifulSoup will extract the resulting data.
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from bs4 import BeautifulSoup
import requests
import re
import pickle
import joblib

"""To import google drive, we write this code in code section."""

from google.colab import drive
drive.mount('/content/drive')

"""1. To access data from the CSV file, we require a function read_csv() that retrieves data in the form of the Dataframe.Here our dataset is website_classification.csv.
2. The call website_df = website_df.sample(frac = 1) "shuffles" the data in series.
3. Using fraction to get a random sample:- By using fraction between 0 to 1, it returns the approximate number of the fraction of the dataset. For example, 0.1 returns 10% of the rows.
"""

website_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/website_classification.csv')
website_df = website_df.sample(frac = 1)
website_df.head()

"""Pandas Index.value_counts() function returns object containing counts of unique values. Here we get the count for the categories of websites."""

website_df["Category"].value_counts().head(30)

"""To train and test the data we need to divide our data into features (X) and labels (y).Here our x is the "cleaned_website_text" from the datset used to predict the weebsite category and y are the "categories" of the websites which is to be predicted."""

x = np.array(website_df["cleaned_website_text"])
y = np.array(website_df["Category"])

print(x)

"""Here we train the machine learning model by providing the samples with Catgeory labels on them and let it gradually figure out how features relate to labels.
1. Each unique word is represented by a column in the matrix that is created by CountVectorizer, and each sample of text from the document is represented by a row in the matrix. Each cell's value is just the occurence of word in that specific text sample. This is our input feature.
2. The fit_transform() function turns the dataset into a count matrix.
3. We use Multinomial Naive Bayes algorithm which is a Machine Learning algorithm most frequently used in NLP for text classification.

model_fit() : fit training data. For supervised learning applications, this accepts two arguments: the data X and the labels y (e.g. model. fit(X, y) ).
"""

#cv = CountVectorizer()
cv = CountVectorizer(token_pattern=r'\b[^\d\W]+\b',binary=True)
X = cv.fit_transform(x)
#print(cv.get_feature_names_out()[0:1000])
#print(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=50)
model = MultinomialNB()
model.fit(X_train,y_train)

# joblib.dump(cv, 'WebVectorizier.pkl', compress=3)
# joblib.dump(model, 'Webclassifier.pkl', compress=3)

with open('WebVectorizier.pkl','wb') as f:
    pickle.dump(cv,f)

with open('Webclassifier.pkl','wb') as f:
    pickle.dump(model,f)

"""1. Here we take the website url as input.
2. We extract the domain part from url as parsed url
3. Then we send a request to the url and get a HTML wepage in response
4. Beautiful Soup(bs4) is a Python library for pulling data out of the received HTML wepage
5. We extract meta-data from wepage and search for description attribute
6. If description attribute is not in meta-data response then we split the url itself into keywords and send the text to model for prediction

"""

def predictWebsiteCatgerory( website_input):
  try:
    print("Input : "+website_input)
    website_split = website_input.split('/', 3)
    website_input = '/'.join(website_split[:3])
    print("Parsed URL : "+website_input)
    headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',
    }
    r = requests.get(website_input, headers=headers)
    soup = BeautifulSoup(r.content, "html")
    meta = soup.find_all('meta')
    # print(r.text)
    for tag in meta:
      if 'name' in tag.attrs.keys() and tag.attrs['name'].strip().lower() == 'description':
          print("Wesbite Description : "+tag.attrs['content'])
          data = cv.transform([tag.attrs['content']]).toarray()
          output = model.predict(data)
          print(output)
          return output

    content = ''
    for link in soup.findAll("a", href=True):
      content = content + link.text

    # print("Checking alternate content from links: "+content)
    if content:
          print("Could not fetch meta data. Checking alternate content from anchor tags....")
          data = cv.transform([content]).toarray()
          output = model.predict(data)
          print(output)
          return output

    website_input = website_input.split('//')[1]
    website_split = re.split(r"[^a-zA-Z\s]", website_input)
    website_text = ' '.join(website_split)
    data = cv.transform([website_text]).toarray()
    output = model.predict(data)
    return output
  except:
    return "Cannot Access Website"

website_input=input("Enter a website: ")
output = predictWebsiteCatgerory(website_input)
print(output)

"""1. We get the browsing history of the user from "**C:\Users\<username>\AppData\Local\Google\Chrome\User Data\Default\History**".
2. This browser history contains the table's "url" and "visit_count".
3. On the basis of the visit_count we then sort the urls in descending order.
"""

user_history = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TrainingHistory.csv')
user_history = user_history.iloc[:,[0,4]]
user_history.columns =['url','visit_count']
user_history.sort_values('visit_count')
user_history = user_history[~user_history.url.str.contains("host|openshift|ruetoj|colab")]
user_history.head()

"""From the sorted urls then we get the 10 most visited websites urls."""

top_10_website=user_history.head(10)
top_10_website

"""1. Taking those 10 most visited urls we then predict the categories to which each of them belonsgs to.
2. For this we make use of the predictWebsiteCatgerory() function which we have defined earlier for the website category prediction.
3. Using this we predict the categories and print the 10 most visited urls with their categories.
"""

# top_20_website1 = top_20_website.assign(Category=lambda x: (predictWebsiteCatgerory(x['url'])))
top_10_website["Category"] = top_10_website["url"].apply(lambda x: (predictWebsiteCatgerory(x)))

top_10_website